{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet import ResNet101\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, Input\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import Huber\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Store path\n",
    "train_dir = '/home/jwf5/venv/FOODX-251_Dataset/train_set/'\n",
    "val_dir = '/home/jwf5/venv/FOODX-251_Dataset/val_set/'\n",
    "\n",
    "train_df = pd.read_csv('FOODX-251_Dataset/train_labels.csv')\n",
    "train_df['img_name'] = train_dir + train_df['img_name'].astype(str)\n",
    "\n",
    "val_df = pd.read_csv('FOODX-251_Dataset/val_labels.csv')\n",
    "val_df['img_name'] = val_dir + val_df['img_name'].astype(str)\n",
    "train_df.head()\n",
    "tLab = train_df['label'].to_numpy()\n",
    "vLab = val_df['label'].to_numpy()\n",
    "tLab.astype(np.int64)\n",
    "vLab.astype(np.int64)\n",
    "tPath = train_df['img_name'].to_numpy()\n",
    "vPath = val_df['img_name'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: FOODX-251_Dataset/train_TFR\n",
      "- Progress: 100.0%Converting: FOODX-251_Dataset/val_TFR\n",
      "- Progress: 100.0%"
     ]
    }
   ],
   "source": [
    "def print_progress(count, total):\n",
    "    pct_complete = float(count) / total\n",
    "    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def wrap_int64(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def wrap_bytes(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def convert(image_paths , labels, out_path):\n",
    "    # Args:\n",
    "    # image_paths   List of file-paths for the images.\n",
    "    # labels        Class-labels for the images.\n",
    "    # out_path      File-path for the TFRecords output file.\n",
    "\n",
    "    print(\"Converting: \" + out_path)\n",
    "\n",
    "    # Number of images. Used when printing the progress.\n",
    "    num_images = len(image_paths)\n",
    "\n",
    "    # Open a TFRecordWriter for the output-file.\n",
    "    with tf.python_io.TFRecordWriter(out_path) as writer:\n",
    "\n",
    "        # Iterate over all the image-paths and class-labels.\n",
    "        for i, (path, label) in enumerate(zip(image_paths, labels)):\n",
    "            # Print the percentage-progress.\n",
    "            print_progress(count=i, total=num_images-1)\n",
    "\n",
    "            img_bytes = open(path,'rb').read()\n",
    "\n",
    "            # Create a dict with the data we want to save in the\n",
    "            # TFRecords file. You can add more relevant data here.\n",
    "            data = \\\n",
    "            {\n",
    "                'image': wrap_bytes(img_bytes),\n",
    "                'label': wrap_int64(label)\n",
    "            }\n",
    "\n",
    "            # Wrap the data as TensorFlow Features.\n",
    "            feature = tf.train.Features(feature=data)\n",
    "\n",
    "            # Wrap again as a TensorFlow Example.\n",
    "            example = tf.train.Example(features=feature)\n",
    "\n",
    "            # Serialize the data.\n",
    "            serialized = example.SerializeToString()\n",
    "\n",
    "            # Write the serialized data to the TFRecords file.\n",
    "            writer.write(serialized)\n",
    "convert(tPath, tLab, 'FOODX-251_Dataset/train_TFR' )\n",
    "convert(vPath, vLab, 'FOODX-251_Dataset/val_TFR' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`dataset` must produce scalar `DT_STRING` tensors whereas it produces shape (TensorShape([Dimension(28), Dimension(28), Dimension(None)]), TensorShape([])) and types (tf.float32, tf.int32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e2baff946df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_parse_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-e2baff946df2>\u001b[0m in \u001b[0;36msave\u001b[0;34m(dataset, location)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     dataset = dataset.map(tf.io.serialize_tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Reads an image from a file, decodes it into a dense tensor, and resizes it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jwf5/venv/lib/python3.5/site-packages/tensorflow/python/data/experimental/ops/writers.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     57\u001b[0m           \"produces shape {0} and types {1}\".format(\n\u001b[1;32m     58\u001b[0m               \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_legacy_output_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m               dataset_ops.get_legacy_output_types(dataset)))\n\u001b[0m\u001b[1;32m     60\u001b[0m     return gen_experimental_dataset_ops.experimental_dataset_to_tf_record(\n\u001b[1;32m     61\u001b[0m         dataset._variant_tensor, self._filename, self._compression_type)  # pylint: disable=protected-access\n",
      "\u001b[0;31mTypeError\u001b[0m: `dataset` must produce scalar `DT_STRING` tensors whereas it produces shape (TensorShape([Dimension(28), Dimension(28), Dimension(None)]), TensorShape([])) and types (tf.float32, tf.int32)"
     ]
    }
   ],
   "source": [
    "def save(dataset, location='FOODX-251_Dataset/'):\n",
    "#     dataset = dataset.map(tf.io.serialize_tensor)\n",
    "    writer = tf.data.experimental.TFRecordWriter(location)\n",
    "    writer.write(dataset)\n",
    "    return location\n",
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def _parse_function(filename, label):\n",
    "  image_string = tf.read_file(filename)\n",
    "  image_decoded = tf.image.decode_jpeg(image_string)\n",
    "  image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "  return image_resized, label\n",
    "with tf.python_io.TFRecordWriter(\"csv.tfrecords\") as writer:\n",
    "    # A vector of filenames.\n",
    "    filenames = tf.constant([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\"])\n",
    "\n",
    "    # `labels[i]` is the label for the image in `filenames[i].\n",
    "    labels = tf.constant([0, 37])\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    save(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
